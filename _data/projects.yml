# Project template

- name: Object Detection using YOLOv10 and RT-DeTr in AWS
  github: fsaudm/YOLOv10_RT-DeTr_in_AWS 
  description: |
    Implemented object detection in construction sites using YOLOv10 and RT-DeTr architectures with PyTorch and Ultralytics libraries. This project aimed to compare the performance of these two state-of-the-art architectures in terms of speed and accuracy, providing insights into their suitability for real-time object detection tasks, and the inherent tradeoff between accuracy and speed. The models were trained on an AWS SageMaker equipped with GPUs and evaluated on a subset of the *SODA: A large-scale open site object detection dataset for deep learning in construction*. The original dataset can be found [here](https://www.sciencedirect.com/science/article/abs/pii/S0926580522003727). The results and analysis, as well as insightful visualizations of the label distribution and training process are available in the [GitHub repository](https://github.com/fsaudm/YOLOv10_RT-DeTr_in_AWS).

- name: LLMs and other big Deep Learning Models with Nvidia NIMs
  github: fsaudm/Nvidia-nims  
  description: |
    This project taps into Nvidia's Neural Infrastructure Modules (NIMs) and presents two networks to run large deep learning models. The repository includes examples using the newly released Llama 3.1, run in both its 8B and 405B versions, and how to set the models to perform tasks like reasoning and multi-lingual queries. Additionally, for text-to-image generation, the project features StabilityAI's Stable Diffusion XL, wrapped in a simple Python function to generate high-quality images. The provided Jupyter notebooks (`llama3_1.ipynb` and `stable_diffusion.ipynb`) are easy to use, making it straightforward to explore LLMs and diffusion models. Note that you will need an API key from Nvidia, which you can obtain from NVIDIA's API catalog. The full implementation, including code, results, and setup instructions, is available on [GitHub](https://github.com/fsaudm/Nvidia-nims).

- name: GMM and HMM - An implementation from scratch in R
  #link: github.com/fsaudm
  github: fsaudm/statistical-learning/tree/main/Coding4
  description: | # This will include new lines to allow paragraphs
    This is an implementation of a Gaussian Mixture Model using the Expectation-Maximization algorithm, and a Hidden Markov Model through the Baum-Welch and Viterbi algorithms. A detailed, step-by-step R markdown file with the code can be found [here](https://fsaudm.github.io/statistical-learning/Coding4/).

- name: Linear SVM using SGD - Implementation from scratch in R
  #link: github.com/fsaudm
  github: fsaudm/statistical-learning/tree/main/Coding5
  description: | # This will include new lines to allow paragraphs
    This project is an implementation of a very efficient Linear Support Vector Machine that profits from Stochastic Gradient Descent via the Pegasos algorithm, as proposed by Shalev-Shwartz et al. (2011) in their *Primal Estimated sub-GrAdient SOlver for SVM* [paper](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://home.ttic.edu/~nati/Publications/PegasosMPB.pdf).
    You can find the associated R markdown file, with details on the algorithm and the implementation [here](https://fsaudm.github.io/statistical-learning/Coding5/).
  
- name: Walmart Store Sales Forecasting
  #link: github.com/fsaudm
  github: fsaudm/statistical-learning/tree/main/Project2
  description: | # This will include new lines to allow paragraphs
    Data Analysis of historical sales from 45 Walmart stores spread across different regions and forecast of future sales using predictive models: Robust Linear Regression on smoothed data (through SVD), 
    to identify and present quantitative insights through visual displays highlighting the stores and departments projected to have the highest and lowest sales in the upcoming months. 
    You can find an R markdown file with the code and the analysis [here](https://fsaudm.github.io/statistical-learning/Project2/).

- name: A computationally fast online model for accurate prediction of post-disaster traffic conditions
  #link: github.com/fsaudm
  github: fsaudm/Recursive-Estimation-of-Polynomial-Approximation-Kaczmarz-Algorithm
  description: | # This will include new lines to allow paragraphs
    The original research by Professor Hadi Meidani, from the Uncertainty Quantification (UQ) research group introduced the application of the Kaczmarz algorithm for real-time, short-term traffic prediction. The proposed method stands out due to its computational efficiency, adaptability to changing traffic conditions, and its unique capability to swiftly incorporate newly streamed data. By leveraging polynomial approximations and exploring various estimation techniques, the paper achieved robust predictions, specifically for atypical traffic scenarios.
    Expanding upon this foundational work, I sought to critically assess and replicate the methodology, identifying potential areas of refinement. This exploration resulted in the implementation of different ways of ordering the "multivariate vector." The findings highlight the spatial ordering's superior prediction accuracy over the time-based order. This deeper analysis further optimized the real-time traffic prediction in extreme weather conditions using the proposed predictive analysis. You can find this project [here](https://uq.cee.illinois.edu/publication.html#).

- name: Neural Network using NumPy for MNIST
  github: fsaudm/NeuralNet_in_NumPy  
  description: |
    This project showcases the implementation of a fully-connected neural network from scratch using only NumPy. The network is trained on the MNIST dataset, and categorizes digits into one of ten classes (0-9) with 94% accuracy. The network features a four-layer architecture, with a total of 164,013 parameters. All activation and helper functions, such as ReLU, Softmax, and Cross-entropy Loss are implemented from scratch. The forward and backward propagation processes are vectorized, and the weights and biases are updated using the gradient descent algorithm. This project is integrated with Weights & Biases for comprehensive experiment tracking. You can dive into the [full implementation and results on GitHub](https://github.com/fsaudm/NeuralNet_in_NumPy), and explore the training logs and hyperparameter optimization on Weights & Biases embedded in the repo. I recently added a notebook with a similar implementation for Fashion MNIST. 
